---
title: "Distribution of correct answers per question"
author: "Christian Medeiros Adriano"
date: "May 6, 2017"
output: html_document
---

 
<h3>About the data</h3>
Mechanical Turk workers answered questions about different popular open source projects. More specifically, workers were asked if they believed that a set of lines of Java source code were related to a software failure (e.g., null pointer exception). More than 600 workers provided 2580 answers for 129 different question (each question was asked to 20 different workers). The experiment that generated this data is described in this <a href=https://export.arxiv.org/abs/1612.03015>paper</a>  

<h3>About the larger study</h3> 
I am studying how to delegate tasks to workers in a way that I am to identify software faults with more precision while asking fewer questions than in the original study. My approach is to investigate how to predict which answers from which workers have the highest accuracy. Therefore, the my first step is to understand which factors affect answer accuracy, as for example: worker skill, worker confidence on her answers, worker perceived difficulty of questions, and size and complexity of the source code. 

<h3>The goal of this analysis</h3>
I tried to answer the following questions:
<du>
  <li>How is answer accuracy distributed over all the questions asked? </li>
  <li>What are the questions with the most and least accurate answers? </li>
  <li>Do these questions have anything in common? </li>
</du>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE);
#install.packages('dplyr', dependencies = TRUE);
library(ggplot2);
library(dplyr);


#Load Answers into a dataframe
#It also removes invalid input and outliers

  setwd("C://Users//chris//OneDrive//Documentos//GitHub//photinus-analytics//");
  fileName = "answerList_data.csv";
  
  data_all <- read.csv(fileName,header = TRUE,sep=",");
  
  #First I need to look at the outliers or invalid values
  #Invalid age and invalid years of experience
  
  #Remove rows with not available data
  dataf <- data_all [rowSums(is.na(data_all))==0,];
  
  #Remove Invalid age and invalid years of experience
  dataf <- dataf [!dataf$Worker.yearsOfExperience <0,];
  dataf <- dataf [!dataf$Worker.age <18,];##minimum age to participate
  
  #Remove people with no experience in programming
  dataf <- dataf [!dataf$Worker.yearsOfExperience ==0,];
  
  #Remove people whose age doesn´t macht with experience 
  dataf <- dataf [!(dataf$Worker.age - dataf$Worker.yearsOfExperience) <5,];
  
  #Remove outliers
  dataf <- dataf [!dataf$Worker.yearsOfExperience >40,];
  dataf <- dataf [!dataf$Worker.age >80,];
```

Compute the number of correct answers per each of the 129 questions

```{r dataWrangling, echo=FALSE}
#summary (dataf);
cat("Data successfully loaded. Number of entries:",length(dataf[,1]));
    
subsetTPTN_df <-subset(dataf,select= c(Question.ID, TP, TN));
CorrectAnswers <- subsetTPTN_df$TP + subsetTPTN_df$TN;
subsetTPTN_df <- data.frame(subsetTPTN_df);
subsetTPTN_df["CorrectAnswers"]<-CorrectAnswers;

by_questions<- group_by(subsetTPTN_df,Question.ID) ;
summaryTable <- summarize(by_questions,sum(CorrectAnswers));
colnames(summaryTable)<-c("QuestoinID","CorrectAnswers");
```
<h3>Plotting distribution</h3>
```{r plotting, echo=FALSE}
ggplot(summaryTable,aes(x=CorrectAnswers))+geom_histogram(binwidth = 0.25) + ggtitle("Distribution of correct answers per question - true positives and true negatives") + labs(y="question count", x="correct answers per question");
```
<p> Since the shape of the histogram resembles a normal distribuion, I performed a Shapiro-Wilk normality test. This test is based on null-hypothesis that the sample comes from a normal distribution. This means that if I reject the null-hipothesis (p-value<0.05), then there is evidence tha the data does not come from a normal distribution. As shown in the output of the test below, the distribution of correct answers per question is not normally distributed.</p>

```{r shapiro, echo=FALSE}
shapiro.test(summaryTable$CorrectAnswers);

```
<h4> Why did some questions have on average different levels of answer accuracy?</h4>
<p>This is an important reflection if we want to explore the data by filtering. The reason is that filtering can reinforce or offset certain biases already present in the data. Take for instance that some questions might have been answered by a disproportionate number of less skilled workers or workers from certain professions. Therefore, if we apply a filter by skill, we might be favoring certain questions more than others. The problem is that this effect is arbitrary, i.e., another dataset would produce a different result.</p>

<p>With respect to the possible biases, I envisaged at least two sources: variation in worker quality and differences in question difficulty. I will investigate these two factors in the next analyzes. </p>
